{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cb9be1-8606-4615-beff-4045a0ba95de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available columns in the DataFrame:\n",
      "Index(['Municipality', 'Block', 'Lot', 'Qual', 'Property Location',\n",
      "       'Property Class', 'Owner's Name', 'Owner's Mailing Address',\n",
      "       'City/State/Zip', 'Sq. Ft.', 'Yr. Built', 'Building Class',\n",
      "       'Prior Block', 'Prior Lot', 'Prior Qual', 'Updated', 'Zone', 'Account',\n",
      "       'Mortgage Account', 'Bank Code', 'Sp Tax Cd', 'Sp Tax Cd.1',\n",
      "       'Sp Tax Cd.2', 'Sp Tax Cd.3', 'Map Page', 'Additional Lots',\n",
      "       'Land Desc', 'Building Desc', 'Class 4 Code', 'Acreage', 'EPL Own',\n",
      "       'EPL Use', 'EPL Desc', 'EPL Statute', 'EPL Init', 'EPL Further',\n",
      "       'EPL Facility Name', 'Taxes 1', 'Taxes 2', 'Taxes 3', 'Taxes 4',\n",
      "       'Sale Date', 'Deed Book', 'Deed Page', 'Sale Price', 'NU Code', 'Ratio',\n",
      "       'Type/Use', 'Year', 'Owner', 'Street', 'City/State/Zip.1',\n",
      "       'Land Assmnt', 'Building Assmnt', 'Exempt', 'Total Assmnt', 'Assessed',\n",
      "       'Year.1', 'Owner.1', 'Street.1', 'City/State/Zip.2', 'Land Assmnt.1',\n",
      "       'Building Assmnt.1', 'Exempt.1', 'Total Assmnt.1', 'Assessed.1',\n",
      "       'Year.2', 'Owner.2', 'Street.2', 'City/State/Zip.3', 'Land Assmnt.2',\n",
      "       'Building Assmnt.2', 'Exempt.2', 'Total Assmnt.2', 'Assessed.2',\n",
      "       'Year.3', 'Owner.3', 'Street.3', 'City/State/Zip.4', 'Land Assmnt.3',\n",
      "       'Building Assmnt.3', 'Exempt.3', 'Total Assmnt.3', 'Assessed.3',\n",
      "       'Latitude', 'Longitude', 'Neigh', 'VCS', 'StyDesc', 'Style', 'Join',\n",
      "       'Unnamed: 91'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with low_memory=False\n",
    "file_path = \"../data/commericalnj.csv\"\n",
    "df2 = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Display the first few rows\n",
    "df2.head()\n",
    "\n",
    "# Print the available columns in the DataFrame\n",
    "print(\"\\nAvailable columns in the DataFrame:\")\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdd022-2b36-4f2a-9208-b88af0e24cd6",
   "metadata": {},
   "source": [
    "\n",
    "# Scalable Prototype for Commercial Property Price Prediction Using Spark\n",
    "\n",
    "This notebook demonstrates how we scale our machine learning prototype to large datasets using Apache Spark. In this prototype, we use Spark ML to build a regression pipeline with a Gradient-Boosted Tree (GBT) model. We also explain the trade-offs of distributed computing and address common schema issues with CSV data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup Spark and Load Data\n",
    "\n",
    "We first install PySpark and create a SparkSession. We then load our CSV file (which contains our commercial property data) with the header and inferred schema.\n",
    "\n",
    "```python\n",
    "!pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sin, cos, lit, log1p, expr\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CommercialPropertyScaling\").getOrCreate()\n",
    "\n",
    "# Adjust file path as needed\n",
    "file_path = \"../data/commericalnj.csv\"\n",
    "df_spark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Select Only the Needed Columns\n",
    "\n",
    "Our CSV file has extra columns and slight differences in header names (for example, the header uses `Total Assmnt` rather than `Total Assmnt55`). To avoid schema mismatches and header warnings, we explicitly select only the columns needed for our model. We then rename columns that contain spaces or punctuation.\n",
    "\n",
    "```python\n",
    "# Select only the necessary columns from the CSV.\n",
    "df_spark = df_spark.select(\n",
    "    \"Municipality\", \n",
    "    \"Block\", \n",
    "    \"Lot\", \n",
    "    \"Qual\", \n",
    "    \"Property Location\", \n",
    "    \"Property Class\", \n",
    "    \"Sq. Ft.\", \n",
    "    \"Yr. Built\", \n",
    "    \"Acreage\", \n",
    "    \"Total Assmnt\",   # Select the header as it appears in the CSV\n",
    "    \"Taxes 1\", \n",
    "    \"Sale Date\", \n",
    "    \"Sale Price\", \n",
    "    \"Type/Use\", \n",
    "    \"Neigh\", \n",
    "    \"Latitude\", \n",
    "    \"Longitude\"\n",
    ")\n",
    "\n",
    "# Rename problematic columns to remove spaces and punctuation.\n",
    "df_spark = df_spark.withColumnRenamed(\"Sq. Ft.\", \"Sq_Ft\") \\\n",
    "                   .withColumnRenamed(\"Yr. Built\", \"Yr_Built\") \\\n",
    "                   .withColumnRenamed(\"Total Assmnt\", \"Total_Assmnt\") \\\n",
    "                   .withColumnRenamed(\"Taxes 1\", \"Taxes_1\")\n",
    "```\n",
    "\n",
    "*Note:* Although Spark warns that the header does not conform to the schema (because of extra columns or slight naming differences), explicitly selecting only the needed columns and renaming them ensures consistency throughout our code.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Cleaning & Feature Engineering\n",
    "\n",
    "We now perform several data cleaning and feature engineering steps:\n",
    "\n",
    "- **Filtering:** We remove transactions with a sale price below \\$500K and remove extreme outliers (keeping only properties at or below the 95th percentile of sale price).\n",
    "- **Date Features:** We convert the \"Sale Date\" column to a timestamp and extract the sale year and month. We then create cyclic features (sine and cosine) for the sale month to capture seasonality.\n",
    "- **Building Age:** We calculate the building age using the \"Yr_Built\" column.\n",
    "- **Target Variable:** We create a log-transformed target variable (`Log_Sale_Price`) to help stabilize variance.\n",
    "\n",
    "```python\n",
    "# Filter out transactions with a Sale Price below $500K.\n",
    "df_spark = df_spark.filter(col(\"Sale Price\") >= 500000)\n",
    "\n",
    "# Remove extreme outliers: keep properties at or below the 95th percentile.\n",
    "quantiles = df_spark.approxQuantile(\"Sale Price\", [0.95], 0.05)\n",
    "if quantiles:\n",
    "    upper_threshold = quantiles[0]\n",
    "    df_spark = df_spark.filter(col(\"Sale Price\") <= upper_threshold)\n",
    "else:\n",
    "    print(\"Warning: No quantiles computed; check your 'Sale Price' column.\")\n",
    "\n",
    "# Convert Sale Date to timestamp and extract date features.\n",
    "df_spark = df_spark.withColumn(\"Sale_Date\", col(\"Sale Date\").cast(\"timestamp\"))\n",
    "df_spark = df_spark.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month\", month(col(\"Sale_Date\")))\n",
    "# Create cyclic features for Sale Month.\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Sine\", sin(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Cosine\", cos(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "\n",
    "# Compute Building Age using the renamed \"Yr_Built\" column.\n",
    "current_year = 2023\n",
    "df_spark = df_spark.withColumn(\"Building_Age\", lit(current_year) - col(\"Yr_Built\"))\n",
    "\n",
    "# Create the log-transformed target variable.\n",
    "df_spark = df_spark.withColumn(\"Log_Sale_Price\", log1p(col(\"Sale Price\")))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Selection & Categorical Indexing\n",
    "\n",
    "We define our feature lists:\n",
    "\n",
    "- **Numerical features:** Use the renamed columns.\n",
    "- **Categorical features:** We use columns such as \"Municipality\", \"Property Class\", \"Type/Use\", and \"Neigh\".\n",
    "\n",
    "We then create a pipeline for encoding the categorical features and assembling all features into a single vector. We set `handleInvalid=\"skip\"` in the `VectorAssembler` to drop any rows that have null values in the feature columns.\n",
    "\n",
    "```python\n",
    "# Define numerical features.\n",
    "numerical_cols = [\"Sq_Ft\", \"Acreage\", \"Building_Age\", \"Sale_Year\", \n",
    "                  \"Sale_Month_Sine\", \"Sale_Month_Cosine\", \"Latitude\", \"Longitude\", \"Total_Assmnt\", \"Taxes_1\"]\n",
    "\n",
    "# Define categorical features.\n",
    "categorical_cols = [\"Municipality\", \"Property Class\", \"Type/Use\", \"Neigh\"]\n",
    "\n",
    "# Create StringIndexers for categorical features.\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"skip\") for col in categorical_cols]\n",
    "\n",
    "# Create OneHotEncoders for the indexed categorical features.\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_OHE\") for indexer in indexers]\n",
    "\n",
    "# Assemble all features into a single vector.\n",
    "assembler_inputs = numerical_cols + [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Define the Regression Model\n",
    "\n",
    "We use Sparkâ€™s Gradient-Boosted Tree Regressor (GBTRegressor) to predict the log-transformed sale price.\n",
    "\n",
    "```python\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Log_Sale_Price\", maxIter=100)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Create the Pipeline\n",
    "\n",
    "We build a pipeline that includes the indexing, encoding, feature assembling, and regression model.\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Split the Data into Training and Test Sets\n",
    "\n",
    "We split our data with an 80/20 ratio.\n",
    "\n",
    "```python\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Train the Model\n",
    "\n",
    "We train our pipeline on the training data.\n",
    "\n",
    "```python\n",
    "model = pipeline.fit(train_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Evaluate the Model\n",
    "\n",
    "We evaluate our model by computing the RMSE on the log scale and then converting predictions back to the dollar scale. We also compute the RÂ² value.\n",
    "\n",
    "```python\n",
    "# Generate predictions on the test data.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate RMSE in log scale.\n",
    "evaluator_log = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_log = evaluator_log.evaluate(predictions)\n",
    "print(\"Test RMSE (log scale):\", rmse_log)\n",
    "\n",
    "# Convert predictions back to dollar scale.\n",
    "predictions = predictions.withColumn(\"Predicted_Sale_Price\", expr(\"exp(prediction) - 1\"))\n",
    "predictions = predictions.withColumn(\"Actual_Sale_Price\", col(\"Sale Price\"))\n",
    "evaluator_dollar = RegressionEvaluator(labelCol=\"Actual_Sale_Price\", predictionCol=\"Predicted_Sale_Price\", metricName=\"rmse\")\n",
    "rmse_dollar = evaluator_dollar.evaluate(predictions)\n",
    "print(\"Test RMSE (dollar scale):\", rmse_dollar)\n",
    "\n",
    "# Optionally, compute RÂ² on the log scale.\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_log = evaluator_r2.evaluate(predictions)\n",
    "print(\"Test RÂ² (log scale):\", r2_log)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Trade-offs and Discussion\n",
    "\n",
    "In your project documentation, explain the following trade-offs:\n",
    "- **Scalability:**  \n",
    "  Spark enables processing of billions of rows via distributed computing. However, it introduces overhead (e.g., data partitioning and shuffling) compared to a single-machine approach.\n",
    "- **Schema Challenges:**  \n",
    "  Real-world CSV files often have inconsistent headers. We explicitly selected and renamed columns to ensure a consistent schema.\n",
    "- **Algorithm Choice:**  \n",
    "  We selected the GBTRegressor from Spark ML because it can scale across large datasets, even though tuning options may differ from those available in single-machine libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Stop the SparkSession\n",
    "\n",
    "Always stop your SparkSession at the end of your processing to release resources.\n",
    "\n",
    "```python\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e3d17f-69f4-40d4-b668-c720bde5a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is set to: /opt/conda\n",
      "/opt/conda/bin/java\n",
      "openjdk version \"11.0.26-internal\" 2025-01-21\n",
      "OpenJDK Runtime Environment (build 11.0.26-internal+0-adhoc..src)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.26-internal+0-adhoc..src, mixed mode)\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.11/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/20 23:31:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/20 23:31:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "25/02/20 23:31:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/02/20 23:33:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (log scale): 0.5123740278419254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 23:33:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "[Stage 1020:>                                                       (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (dollar scale): 3643790.7587949852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 23:33:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt, Latitude, Longitude, Neigh\n",
      " Schema: Municipality, Block, Lot, Qual, Property Location, Property Class, Sq. Ft., Yr. Built, Acreage, Taxes 1, Sale Date, Sale Price, Type/Use, Total Assmnt55, Latitude, Longitude, Neigh\n",
      "Expected: Total Assmnt55 but found: Total Assmnt\n",
      "CSV file: file:///home/sagemaker-user/realestate-tool-final/data/commericalnj.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RÂ² (log scale): 0.6477173695635902\n",
      "\n",
      "Trade-offs and Discussion:\n",
      " - Distributed computing with Spark enables processing large datasets but introduces overhead from data partitioning.\n",
      " - The chosen GBTRegressor, while scalable, may have tuning trade-offs compared to single-machine algorithms.\n",
      " - Explicit column renaming and feature engineering help maintain schema consistency.\n"
     ]
    }
   ],
   "source": [
    "# Set JAVA_HOME and update PATH so Spark can find the Java executable.\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/conda\"  # Use /opt/conda since 'which java' returns /opt/conda/bin/java\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "print(\"JAVA_HOME is set to:\", os.environ.get(\"JAVA_HOME\"))\n",
    "!which java\n",
    "!java -version\n",
    "\n",
    "# Install pyspark if needed\n",
    "!pip install pyspark\n",
    "\n",
    "# Import required libraries from pyspark and Python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sin, cos, lit, log1p, expr\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CommercialPropertyScaling\").getOrCreate()\n",
    "\n",
    "# Adjust file path as needed and load the CSV file\n",
    "file_path = \"../data/commericalnj.csv\"\n",
    "df_spark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Rename problematic columns to remove spaces and punctuation\n",
    "df_spark = df_spark.withColumnRenamed(\"Sq. Ft.\", \"Sq_Ft\") \\\n",
    "                   .withColumnRenamed(\"Yr. Built\", \"Yr_Built\") \\\n",
    "                   .withColumnRenamed(\"Total Assmnt55\", \"Total_Assmnt\") \\\n",
    "                   .withColumnRenamed(\"Taxes 1\", \"Taxes_1\")\n",
    "\n",
    "# Select only the needed columns using the renamed names\n",
    "df_spark = df_spark.select(\n",
    "    \"Municipality\", \n",
    "    \"Block\", \n",
    "    \"Lot\", \n",
    "    \"Qual\", \n",
    "    \"Property Location\", \n",
    "    \"Property Class\", \n",
    "    \"Sq_Ft\", \n",
    "    \"Yr_Built\", \n",
    "    \"Acreage\", \n",
    "    \"Total_Assmnt\", \n",
    "    \"Taxes_1\", \n",
    "    \"Sale Date\", \n",
    "    \"Sale Price\", \n",
    "    \"Type/Use\", \n",
    "    \"Neigh\", \n",
    "    \"Latitude\", \n",
    "    \"Longitude\"\n",
    ")\n",
    "\n",
    "# Data Cleaning & Feature Engineering\n",
    "# Filter out transactions with a Sale Price below $500K.\n",
    "df_spark = df_spark.filter(col(\"Sale Price\") >= 500000)\n",
    "\n",
    "# Remove extreme outliers: keep properties at or below the 95th percentile.\n",
    "quantiles = df_spark.approxQuantile(\"Sale Price\", [0.95], 0.05)\n",
    "if quantiles:\n",
    "    upper_threshold = quantiles[0]\n",
    "    df_spark = df_spark.filter(col(\"Sale Price\") <= upper_threshold)\n",
    "else:\n",
    "    print(\"Warning: No quantiles computed; check your 'Sale Price' column.\")\n",
    "\n",
    "# Convert 'Sale Date' to timestamp and extract year and month.\n",
    "df_spark = df_spark.withColumn(\"Sale_Date\", col(\"Sale Date\").cast(\"timestamp\"))\n",
    "df_spark = df_spark.withColumn(\"Sale_Year\", year(col(\"Sale_Date\")))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month\", month(col(\"Sale_Date\")))\n",
    "# Create cyclic features for the sale month.\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Sine\", sin(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "df_spark = df_spark.withColumn(\"Sale_Month_Cosine\", cos(2 * 3.14159 * col(\"Sale_Month\") / 12))\n",
    "\n",
    "# Compute Building Age using the renamed \"Yr_Built\" column.\n",
    "current_year = 2023  # Adjust programmatically if needed\n",
    "df_spark = df_spark.withColumn(\"Building_Age\", lit(current_year) - col(\"Yr_Built\"))\n",
    "\n",
    "# Create the log-transformed target variable for stability.\n",
    "df_spark = df_spark.withColumn(\"Log_Sale_Price\", log1p(col(\"Sale Price\")))\n",
    "\n",
    "# Feature Selection & Categorical Indexing\n",
    "# Define numerical and categorical columns.\n",
    "numerical_cols = [\"Sq_Ft\", \"Acreage\", \"Building_Age\", \"Sale_Year\", \n",
    "                  \"Sale_Month_Sine\", \"Sale_Month_Cosine\", \"Latitude\", \"Longitude\", \"Total_Assmnt\", \"Taxes_1\"]\n",
    "categorical_cols = [\"Municipality\", \"Property Class\", \"Type/Use\", \"Neigh\"]\n",
    "\n",
    "# Create StringIndexers for categorical features.\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"skip\") for col in categorical_cols]\n",
    "# Create OneHotEncoders for indexed features.\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_OHE\") for indexer in indexers]\n",
    "\n",
    "# Assemble all features into a single vector.\n",
    "assembler_inputs = numerical_cols + [encoder.getOutputCol() for encoder in encoders]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# Define the regression model using Spark's GBTRegressor.\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Log_Sale_Price\", maxIter=100)\n",
    "\n",
    "# Create the pipeline by combining all stages.\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model.\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Evaluate the model.\n",
    "predictions = model.transform(test_data)\n",
    "evaluator_log = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_log = evaluator_log.evaluate(predictions)\n",
    "print(\"Test RMSE (log scale):\", rmse_log)\n",
    "\n",
    "# Convert predictions back to the original sale price scale.\n",
    "predictions = predictions.withColumn(\"Predicted_Sale_Price\", expr(\"exp(prediction) - 1\"))\n",
    "predictions = predictions.withColumn(\"Actual_Sale_Price\", col(\"Sale Price\"))\n",
    "evaluator_dollar = RegressionEvaluator(labelCol=\"Actual_Sale_Price\", predictionCol=\"Predicted_Sale_Price\", metricName=\"rmse\")\n",
    "rmse_dollar = evaluator_dollar.evaluate(predictions)\n",
    "print(\"Test RMSE (dollar scale):\", rmse_dollar)\n",
    "\n",
    "# Optionally, compute RÂ² on the log scale.\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Log_Sale_Price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_log = evaluator_r2.evaluate(predictions)\n",
    "print(\"Test RÂ² (log scale):\", r2_log)\n",
    "\n",
    "# Trade-offs and Discussion (this section is printed to explain your design choices)\n",
    "print(\"\\nTrade-offs and Discussion:\")\n",
    "print(\" - Distributed computing with Spark enables processing large datasets but introduces overhead from data partitioning.\")\n",
    "print(\" - The chosen GBTRegressor, while scalable, may have tuning trade-offs compared to single-machine algorithms.\")\n",
    "print(\" - Explicit column renaming and feature engineering help maintain schema consistency.\")\n",
    "\n",
    "# Stop the SparkSession to free resources.\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a95a1-749a-47dc-acd3-58df71563590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
